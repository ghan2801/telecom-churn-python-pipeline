# ğŸ“Š Telecom Customer Churn â€“ Python ELT Pipeline

This project implements a complete ELT (Extract, Load, Transform) pipeline using **pure Python** for processing and analyzing telecom customer churn data. It includes data ingestion, cleaning, anonymization, storage, scheduling, and reporting via Excel and a Streamlit dashboard.

---

## ğŸ“‚ Folder Structure

telecom-churn-python-pipeline/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ telecom_churn.csv # Raw dataset
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ transform_and_load.py # Main ETL logic
â”‚ â”œâ”€â”€ orchestrator.py # Runs ETL every hour using schedule
â”‚ â””â”€â”€ reporting_app.py # Streamlit dashboard app
â”œâ”€â”€ processed_telecom_churn.db # Output SQLite DB
â”œâ”€â”€ processed_churn_report.xlsx # Excel report
â”œâ”€â”€ requirements.txt # Python package dependencies
â””â”€â”€ README.md


---

## âœ… Features

-  Loads customer data from CSV
-  Anonymizes sensitive PII (CustomerID)
-  Cleans missing values in key fields
-  Loads processed data to SQLite DB
-  Exports cleaned data to Excel
-  Generates an interactive dashboard with Streamlit
-  Schedules the ETL to run hourly using `schedule`

---

## ğŸ› ï¸ Tech Stack

- Python (pandas, sqlalchemy, openpyxl)
- SQLite (lightweight local database)
- Streamlit (for dashboard)
- schedule (for hourly orchestration)
- Excel (for offline report)

---

## ğŸš€ How to Run This Project

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/telecom-churn-python-pipeline.git
cd telecom-churn-python-pipeline

### 2. Install Python Dependencies
pip install -r requirements.txt

### 3. Run ETL Pipeline

python scripts/transform_and_load.py
 Output:

processed_telecom_churn.db â€“ cleaned data in SQLite

processed_churn_report.xlsx â€“ Excel file for offline analysis

4. View the Dashboard 
streamlit run scripts/reporting_app.py
Open http://localhost:8501 in your browser.

5. Run ETL Every Hour (Simulated Orchestration)
  
python scripts/orchestrator.py
This runs transform_and_load.py once immediately and then every hour using the schedule library.

